{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":46697,"sourceType":"datasetVersion","datasetId":31559}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import ViTFeatureExtractor, TFViTModel\nfrom PIL import Image\nimport numpy as np\nimport os\n\ntrain_dir = \"/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/train\"\ntest_dir = \"/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/test\"\nimg_size = (224, 224)\nbatch_size = 32\ncheckpoint_path = \"./best_model_checkpoint.h5\"\n\nclasses = sorted(os.listdir(train_dir))\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n\ndef load_and_preprocess_image(img_path, label):\n    if not tf.io.gfile.exists(img_path) or tf.io.gfile.isdir(img_path):\n        raise ValueError(f\"{img_path} is not a valid file.\")\n\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.resize(img, img_size)\n    img = tf.cast(img, tf.float32) / 255.0  \n    \n    img_np = img.numpy()\n    pil_image = Image.fromarray((img_np * 255).astype(np.uint8))\n\n    pixel_values = feature_extractor(images=pil_image, return_tensors='np')['pixel_values']\n    pixel_values = tf.convert_to_tensor(pixel_values, dtype=tf.float32)  \n    pixel_values = tf.squeeze(pixel_values, axis=0)  \n\n    return pixel_values, label  \n\ndef preprocess_dataset(file_paths, labels, batch_size, shuffle=False):\n    def gen():\n        for path, label in zip(file_paths, labels):\n            try:\n                yield load_and_preprocess_image(path, label)\n            except Exception as e:\n                print(f\"Skipping file {path} due to error: {e}\")\n    \n    dataset = tf.data.Dataset.from_generator(\n        gen,\n        output_signature=(\n            tf.TensorSpec(shape=(3, 224, 224), dtype=tf.float32),\n            tf.TensorSpec(shape=(), dtype=tf.int64)\n        )\n    )\n    \n    dataset_length = len(file_paths)\n    print(\"Dataset length:\", dataset_length)\n    \n    if shuffle and dataset_length > 0:\n        dataset = dataset.shuffle(buffer_size=min(dataset_length, 10000))  \n    dataset = dataset.batch(batch_size)\n\n    for image_batch, label_batch in dataset.take(1):\n        tf.print(\"Dataset image batch shape:\", tf.shape(image_batch))\n        tf.print(\"Dataset label batch shape:\", tf.shape(label_batch))\n    \n    return dataset\n\ndef get_file_paths_and_labels(directory):\n    file_paths = []\n    labels = []\n    for class_name in os.listdir(directory):\n        class_dir = os.path.join(directory, class_name)\n        if os.path.isdir(class_dir):  \n            for fname in os.listdir(class_dir):\n                full_path = os.path.join(class_dir, fname)\n                if os.path.isfile(full_path):  \n                    file_paths.append(full_path)\n                    labels.append(classes.index(class_name))\n    return file_paths, labels\n\ntrain_file_paths, train_labels = get_file_paths_and_labels(train_dir)\ntest_file_paths, test_labels = get_file_paths_and_labels(test_dir)\n\ntrain_ds = preprocess_dataset(train_file_paths, train_labels, batch_size, shuffle=True)\n\ntotal_size = len(test_file_paths)\nval_size = 4020\ntest_size = total_size - val_size\n\ntest_file_paths, test_labels = zip(*list(zip(test_file_paths, test_labels)))\ntest_ds = preprocess_dataset(test_file_paths, test_labels, batch_size, shuffle=False)\nval_ds = test_ds.take(val_size)\ntest_ds = test_ds.skip(val_size)\n\nbase_model = TFViTModel.from_pretrained('google/vit-base-patch16-224')\n\ndef create_model():\n    inputs = tf.keras.Input(shape=(3, 224, 224), dtype=tf.float32)\n    \n    features = base_model(inputs)[0]  \n    \n    pooled_features = tf.keras.layers.GlobalAveragePooling1D()(features)  \n    \n    num_classes = len(classes)\n    logits = tf.keras.layers.Dense(num_classes, activation='softmax')(pooled_features)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=logits)\n    return model\n\n\nmodel = create_model()\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    monitor='val_loss',\n    save_best_only=True,\n    save_weights_only=True,\n    mode='min'\n)\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=20,\n    callbacks=[checkpoint_callback]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T16:18:07.510622Z","iopub.execute_input":"2024-09-11T16:18:07.511188Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91184dc400bb41488041225b1cefe1ff"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Dataset length: 8144\nDataset image batch shape: [32 3 224 224]\nDataset label batch shape: [32]\nDataset length: 8041\nDataset image batch shape: [32 3 224 224]\nDataset label batch shape: [32]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ba2bcbfd73a4d33af67cf32ce4b7fdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"230b6da327c84c63bc431dc8db63f45f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTModel: ['classifier.bias', 'classifier.weight']\n- This IS expected if you are initializing TFViTModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFViTModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFViTModel were not initialized from the PyTorch model and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1726071845.148116     101 service.cc:145] XLA service 0x7a82f066fb40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1726071845.148173     101 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1726071845.297695     101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"255/255 [==============================] - 528s 1s/step - loss: 4.6723 - accuracy: 0.0720 - val_loss: 3.6793 - val_accuracy: 0.2140\nEpoch 2/20\n255/255 [==============================] - 391s 1s/step - loss: 2.4934 - accuracy: 0.5076 - val_loss: 2.3123 - val_accuracy: 0.5032\nEpoch 3/20\n255/255 [==============================] - 404s 1s/step - loss: 1.0206 - accuracy: 0.8745 - val_loss: 1.7093 - val_accuracy: 0.6218\nEpoch 4/20\n255/255 [==============================] - 390s 1s/step - loss: 0.3755 - accuracy: 0.9786 - val_loss: 1.5313 - val_accuracy: 0.6505\nEpoch 5/20\n255/255 [==============================] - 390s 1s/step - loss: 0.1757 - accuracy: 0.9952 - val_loss: 1.3876 - val_accuracy: 0.6785\nEpoch 6/20\n255/255 [==============================] - 391s 1s/step - loss: 0.0745 - accuracy: 0.9979 - val_loss: 1.3170 - val_accuracy: 0.6942\nEpoch 7/20\n255/255 [==============================] - 393s 1s/step - loss: 0.0481 - accuracy: 0.9978 - val_loss: 1.2810 - val_accuracy: 0.7014\nEpoch 8/20\n255/255 [==============================] - 393s 1s/step - loss: 0.0346 - accuracy: 0.9979 - val_loss: 1.2572 - val_accuracy: 0.7009\nEpoch 9/20\n255/255 [==============================] - 392s 1s/step - loss: 0.0277 - accuracy: 0.9977 - val_loss: 1.2382 - val_accuracy: 0.7055\nEpoch 10/20\n255/255 [==============================] - 392s 1s/step - loss: 0.0227 - accuracy: 0.9977 - val_loss: 1.2183 - val_accuracy: 0.7080\nEpoch 11/20\n255/255 [==============================] - 391s 1s/step - loss: 0.0199 - accuracy: 0.9978 - val_loss: 1.2039 - val_accuracy: 0.7125\nEpoch 12/20\n255/255 [==============================] - 392s 1s/step - loss: 0.0163 - accuracy: 0.9974 - val_loss: 1.1924 - val_accuracy: 0.7116\nEpoch 13/20\n255/255 [==============================] - 390s 1s/step - loss: 0.0169 - accuracy: 0.9974 - val_loss: 1.1978 - val_accuracy: 0.7087\nEpoch 14/20\n255/255 [==============================] - 390s 1s/step - loss: 0.0145 - accuracy: 0.9980 - val_loss: 1.1699 - val_accuracy: 0.7177\nEpoch 15/20\n255/255 [==============================] - 391s 1s/step - loss: 0.0116 - accuracy: 0.9979 - val_loss: 1.1546 - val_accuracy: 0.7183\nEpoch 16/20\n255/255 [==============================] - 390s 1s/step - loss: 0.0107 - accuracy: 0.9975 - val_loss: 1.1460 - val_accuracy: 0.7191\nEpoch 17/20\n255/255 [==============================] - 389s 1s/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 1.1411 - val_accuracy: 0.7204\nEpoch 18/20\n  1/255 [..............................] - ETA: 5:58:33 - loss: 0.0044 - accuracy: 1.0000","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(val_ds)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T22:25:39.101639Z","iopub.execute_input":"2024-09-08T22:25:39.102091Z","iopub.status.idle":"2024-09-08T22:27:47.486763Z","shell.execute_reply.started":"2024-09-08T22:25:39.102021Z","shell.execute_reply":"2024-09-08T22:27:47.485897Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"252/252 [==============================] - 128s 509ms/step - loss: 1.2088 - accuracy: 0.7096\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[1.2087591886520386, 0.7096132040023804]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}